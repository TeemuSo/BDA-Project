---
title: "Heart Failure Prediction using Bayesian Approach"
author: "Teemu Sormunen, Abdullah GÃ¼nay, Nicola Brazzale"
date: "11/18/2020"
header-includes:
  - \usepackage[numbers]{natbib}
# Citations
bibliography: ./citations/sources.bib
# csl: ./citations/vancouver-brackets.csl unnecessary?

output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    citation_package: natbib

---

\newpage

# Introduction

In this project we analyse the data from Heart desease dataset [@origpaper]. We perform a Bayesian analysis of the data using this sequence of operations:

**Overview of analysis problem and of the dataset**
**Data preprocsessing and visualisation**
**Prior choice discussion**
**Models used in our analysis**
**$\hat{R}$ convergence**
**HMC specific convergence diagnostics**
**Effective sample size diagnostic (n_eff or ESS)**
**Model comparison**
**Prior sensitivity analysis**
**Discussion and conclusion**

In this project, we are observing the dataset from two distinct point of views.
On the other hand, we want to do predictive analysis whether patient dies, or survives the research follow up period (4 - 285 days), and on the other hand we want to create an actual survival analysis, which takes time and censored variables in to consideration. For this reason we are inspecting the variables from two points. More about this in upcoming chapters.

## The problem
Cardiovascular Heart Disease (CHD) is the top reason causing 31% of deaths globally. Pakistan is one of the countries where CHD is increasing significantly, and previous studies do not directly apply to Pakistani area due to different diet patterns.
[@origpaper]

## The motivation
With this project we aim to estimate death events and the major risk factors for heart failure with, possibly, high accuracy [@origpaper].

## Modeling idea

We created 4 models which are then compared based on $\hat{r}$, $n_{eff}$, using the loo package and the classification accuracy. The three first models ignore the time feature, and simply predict whether patient has died (1) during the experiment duration, or survived (0). We chose this approach to practise survival analysis with binary outcome.  
We also created fourth model, which predicts the time with respect to death event. In this case, the death event 1 means the patient has died, and death event 0 means that the patient is censored from the study. Censoring practically means, that the patient has opted out of the study, and the researches couldn't reach him anymore. In this context, it doesn't necessarily mean that the patient has survived, but we just don't know the outcome.  

The 1st model is the reduced model and consists in fewer varibles which are selected base on their correlation with the death event. The 2nd model consists in all variables except for the variable "time" as we believe that doesn't represent an important factor in the death event scenario. The 3rd model used is a hierarchical model where we treated age class patients in a group with respect to the other selected variables. 

The 4th model is similar type of linear model, but this time we consider time as outcome variable with respect to death event. We use correlation matrix to select the strongest variable that correspond with time, and we also use BRMS internal function cens() for taking the censored variable DEATH_EVENT to consideration. More clear explanation is given later.  

# Dataset

## Term explanation  

Some of the terms in the dataset might not be familiar, and they are opened briefly here.

* **Creatine phosphokinase (CPK)**  
CPK is an enzyme, which helps to regulate the concentration of adenosine triphosphate (ATP) in cells. ATP is responsible for carrying energy. If the CPK level is high, it often means that there has been an injury or stress on a muscle tissue. Although CPK is one the oldest markers of heart attack, high CPK might also indicate of acute muscle injury along with acute heart problems.  
Normal level of CPK ranges from 20 to 200 IU/L [@phosphokinase]

* **Ejection fraction (EF)**  
EF is a measurement in percentage which describes how much blood left ventricle pumps out of heart with each contraction.
Low EF might indicate potential heart issues.  
Normal EF is 50 to 70 percent, while measurement under 40 percept might be an indicator of heart failure or cardiomyopathy. [@ef]  

* **Platelets**  
Platelets are small cell fragments which can form clots. 
Too many platelets can lead to clotting of blood vessels, 
which in turn can lead to heart attack. Too 
Normal range of platelets is from 150 000 to 450 000. [@platelets]

* **Serum creatinine**  
When creatine breaks down, it forms a waste product called creatinine. Kidneys normally remove creatinine from body. Serum creatinine measures level of creatinine in the blood, indicating the kidney health. High levels of creatinine might indicate a kidney dysfunctioning.  
Normal level of creatinine range from 0.9 to 1.3 mg/dL in men and 0.6 to 1.1 mg/dL in women who are 18 to 60 years old. [@creatinine_serum]

* **Serum sodium**  
Serum sodium measures the amount of sodium in blood. Sodium enters blood through food and drink, and leaves by urine, stool and sweat. Too much sodium can cause blood pressure, while too little sodium can cause nausea, vomiting, exhaustion or dizziness.  
Normal levels of serum sodium are 135 to 145 mEq/L, according to Mayo Clinic. There are however different interpretations of "normal".[@sodium]

* **Time**
Time variable indicates the time since the research has started for that person (the time of ventricular systolic dysfunction). We have time variable included, because we have to inspect when the death events are happening. This variable is ignored in the first three models, because we wanted to also interpret this dataset from binary survival approach, so predict whether patient dies or not.  


## Dataset introduction

The dataset of 299 patients was produced as a result of study [@origpaper] from Pakistani's city Faisalabad. All of the patients were over 40 years old, each having ventricular systolic dysfunction. This means that patient has poor left ventricular ejection fraction. The follow up period was 4 to 285 days, with average of 130 days. This has to be taken in to consideration when doing the survival analysis.  
The dataset has 105 women, and 194 men.
EF, serum creatinine and platelets are categorical variables, and age, serum sodium and CPK are continuous variables.  
Statistical analysis by [@origpaper] found age, creatinine, sodium, anemia and BP as significant variables.  
 
\newpage
# Packages
```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(brms)
library(rstanarm)
library(loo)
library(bayesplot)
library(rstan)
```

Load data
```{r}
file.name <- './data/heart_failure_clinical_records_dataset.csv'
heart <- read_csv(file.name)
```
Prevent text overflow on PDF
```{r}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


\newpage
# Data preprocessing and visualization

## Plot histograms

We are first plotting the histograms to get an overview of the dataset.  

It seems like the sex between ages is distributed quite evenly, there's slightly more patients from the 50-60.

```{r}
ggplot(heart, aes(x=age)) + geom_histogram(aes(fill=as.character(sex)), bins = 30) + labs(fill = "Sex")
```

\newpage
This histogram might suggest us that older people die during this follow up period with higher probability, and younger people either survive or opt-out of the study. 
```{r}
ggplot(heart, aes(x=age)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Creatinine phosphokinase doesn't bring too much information based on the histogram, although it shows us that people with high creatinine phosphokinase might tend to die more often. Because almost everyone who attended this study had already increased phosphokinase levels, we should take this interpretation with a slight grain of salt. 
```{r}
ggplot(heart, aes(x=creatinine_phosphokinase)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Ejection fraction seems to correlate strongly with death. This is logical, because ejection fraction measures the hearts ability to pump blood. Here we can again see that most of the people fall under normal levels of EF.
```{r}
ggplot(heart, aes(x=ejection_fraction)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Platelets seems to follow quite even distribution with respect to death event, and there isn't too much information available only based on the histogram.
```{r}
ggplot(heart, aes(x=platelets)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")
```

\newpage
Serum creatinine seems to correlate quite highly with death. When the upper bound for serum creatinine 1.3 is passed, it seems that probability for death becomes high.
```{r}
ggplot(heart, aes(x=serum_creatinine)) + geom_histogram(aes(fill=as.character(DEATH_EVENT)), bins = 30) + labs(fill = "Death")

```

## Correlation matrix

By plotting the correlation matrix we can see the correlations.

```{r}
pred <- c("high_blood_pressure", "age", "sex", "creatinine_phosphokinase", "diabetes", "ejection_fraction", "platelets", "serum_creatinine", "serum_sodium", "smoking", "anaemia", "time") 
target <- c("DEATH_EVENT")
#formula <- paste("DEATH_EVENT ~", paste(pred, collapse = "+"))
p <- length(pred)
n <- nrow(heart)
x = cor(heart[, c(target,pred)]) 
corrplot(x)
```

Looking at the correlations only by eye might not be enough, so let's list the correlations with respect to death_event in sorted order.

```{r}
sort(x[1,])
```

We see that highly positively correlating variables are age and serum creatinine, which found to be true already earlier. Strongest negative correlations are time, ejection fraction and serum sodium. Time clearly indicates that people tend to die early after the ventricular systolic dysfunction has happened. Serum sodium also seems to correlate negatively, because the low sodium levels are usually occurring after heart failure. If we have to make conclusions, then we could conclude that the sodium levels are lower in the people who have had more severe ventricular systolic dysfunction.  

For the fourth survival analysis model we should also look at the highest correlations according to time. 

```{r}
sort(x[nrow(x),])
```

According to time we see that highest negative correlation when death_event is discarded are age, high blood pressure, serum creatinine and anemia. This means that old people tend to die early, with high blood pressure, with high serum creatinine, with anemia. This seems to somewhat run hand in hand with the previous conclusions, although this time high blood pressure and anemia was introduced. High blood pressure sounds like it could lead to heart attack easily, which makes sense. Also anemia seems to go hand in hand with heart attack.  


# Models

We chose to use BRMS for modeling. It stands for Bayesian Regression Models for Stan. It's an interface to fit Bayesian generalized (non-)linear multivariate models using Stan. As we need to do analysis for binary response variables, we need some sort of a generalized linear model. We also chose BRMS due to its ease of use when fitting such complicated multilevel generalized linear models.

In BRMS modeling, the parameters are said to either be population level or group level. Population-level parameters means the same thing as regular parameters in our course, and group-level parameters mean hyperparameters in hierarchical case.  

**Family argument** specifies the distribution family of the response variable.    

**Prior argument** for each of the parameters. One can set different priors for each population level parameter, or group level parameter.

Before creating the test/train datasets, we need to preprocess the data little bit. Ejection fraction is described as percentage, and it can be given prior with beta distribution, which is constrained from 0 to 1. Let's normalize ejection fraction to be from 0 to 1.
```{r}
heart$ejection_fraction = heart$ejection_fraction / 100
```

Create general function for splitting the train and test data.
```{r}
split.train.test <- function(data, test.size = 0.3) {
  train.indice <- sample(nrow(heart), nrow(heart)*(1-test.size))
  train.data <- heart[train.indice,]
  test.data <- heart[-train.indice,]
  return(list("train" = train.data, "test" = test.data))
}
new.data <- split.train.test(heart)
train.data <- new.data$train
test.data <- new.data$test
```

## Model fitting

The Generalised Linear Model used for every parametrisation is Bernoulli-Logit Generalised Linear Model for the first three models, which is logistic regression. It's expressed in mathematical terms as following:  

$BernoulliLogitGLM(y|x,\alpha, \beta) = \prod_{1\leq i\leq n} Bernoulli(y_i | logit^{-1}(\alpha_i + x_i\times \beta))$



### Full model

Stan code for full model can be found in Appendix A.  
Full model includes all the parameters that are specified in the dataset.

```{r}
fit.full <- brm(formula = DEATH_EVENT ~ age + ejection_fraction + serum_creatinine + serum_sodium + high_blood_pressure + creatinine_phosphokinase + diabetes + smoking + anaemia + sex,
                prior = c(set_prior("cauchy(40,20)", class="b", coef="age"),
                          set_prior("inv_gamma(1,5)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("cauchy(0,200)", class="b", coef="serum_sodium"),
                          set_prior("normal(.5, .5)", class="b", coef="anaemia"),
                          set_prior("normal(.5, .5)", class="b", coef="diabetes"),
                          set_prior("normal(.5, .5)", class="b", coef="smoking"),
                          set_prior("normal(.5, .5)", class="b", coef="high_blood_pressure"),
                          set_prior("normal(.5, .5)", class="b", coef="sex")),
           data = train.data,
           family = bernoulli(),
           refresh=0
)
```

### Feature selected model

Stan code for feature selected model can be found in Appendix B.  

In feature selected model, we hand pick the features that seems to be the most promising with regards to fitting the model. As described above in correlation analysis, we saw that ejection_fraction, serum_creatinine, serum_sodium and age were correlating to death.  

Based on this we can choose these variables to be the important ones, and build a model with them.

```{r}
fit.feature_selected <- brm(formula = DEATH_EVENT ~ ejection_fraction + serum_creatinine + serum_sodium + age,
                            data = train.data,
                            family = bernoulli(),
                          prior = c(set_prior("cauchy(40,20)", class="b", coef="age"),
                          set_prior("inv_gamma(1,1)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("cauchy(0,200)", class="b", coef="serum_sodium")),
                            refresh = 0
                            )
```


### Hierarchical model

Stan code for hierarchical model can be found in Appendix C.  

In hierarchical model, we choose age as hyperparameter, because by intuition we thought that different aged people tend to have different medical conditions by default. This intuition is supported by calculating the absolute row sums of the correlation matrix rows, and seeing absolute correlations of variables. 
```{r}
sort(rowSums(abs(x)))
```
First we will discretize age data in to 3 equal depth bins.


```{r}
discretize.variable <- function(variable.to.discretize, thresholds = c(55, 70)) {
  variable.to.discretize[variable.to.discretize <= thresholds[1]] = 0
  variable.to.discretize[variable.to.discretize > thresholds[1] & variable.to.discretize < thresholds[2]] = 1
  variable.to.discretize[variable.to.discretize >= thresholds[2]] = 2
  return(variable.to.discretize)
}

test.data$age <- discretize.variable(test.data$age)
train.data$age <- discretize.variable(train.data$age)
hist(c(train.data$age, test.data$age))
```

Then we can fit the model
```{r}
fit.hierarchical <- brm(formula = DEATH_EVENT ~ ejection_fraction + serum_creatinine + serum_sodium + (ejection_fraction + serum_creatinine + serum_sodium| age),
           data = train.data,
           family = bernoulli(),
           c(set_prior("cauchy(0,5)", class="b", coef="serum_creatinine"),
                          set_prior("beta(6,4)", class="b", coef="ejection_fraction"),
                          set_prior("normal(100,40)", class="b", coef="serum_sodium")),
           refresh=0,
           control = list(adapt_delta = 0.99)
)
```


### Model for death time analysis  

As a fourth model we have model with response variable according to Weibull distribution, and features are selected based on correlation matrix.  
In death time analysis model we are using different functions based on whether sample is censored or not. If sample is censored, then we are using log probability density function  


$Weibull(y|\alpha, \sigma) = \dfrac{\alpha}{\sigma}\left(\dfrac{y}{\sigma}\right)^{-\alpha-1}exp\left(-\left(\dfrac{y}{\sigma}\right)^{\alpha}\right)$.


If sample is not censored, then we are using the cumulative function of previously mentioned probability density function.  
First we need to retrieve the original data with original age values
```{r}
new.data <- split.train.test(heart)
train.data <- new.data$train
test.data <- new.data$test
```

Then we can fit the model
```{r}
fit.weibull <- brm(formula = time | cens(DEATH_EVENT) ~ age  + anaemia + high_blood_pressure,
           data = train.data,
           family = weibull(),
           prior = c(set_prior("cauchy(40, 20)", class="b", coef="age"),
                     prior_string("normal(.5, .5)", class="b", coef="anaemia"),
                     prior_string("normal(.5, .5)", class="b", coef="high_blood_pressure")),
           refresh=0
)
```

## Prior choices 

We chose priors based on articles that we read about medical measurements. We used weakly informative priors, because there's only little indication in what type of prior we should use. 

### First models

Same priors were used more or less to every of first 3 models, and we will specify them here.  

**Coefficient distributions**  
Age distribution is using half Cauchy distribution, which fits our population well. Cauchy is non-negative continuous distribution, which peaks at around 40, and decades to over 100. This is good, because it forces the prior to be non-negative, which is a must in age distribution. Also it centers around 40, which is convenient, because our age started from 40. 
$age \sim Cauchy(y|\mu,\sigma) = \dfrac{1}{\pi\sigma}\dfrac{1}{1+((y-\mu)/\sigma)^2}$
$age \sim Cauchy(40,20)$

Serum creatinine used inverse gamma distribution. Creatinine levels also need non-negative distribution, and the normal level should be between 0.9 and 1.3. Severe symptoms start when creatinine reaches over 5, so we don't restrict the tail.
$\text{serum creatinine} \sim InvGamma(y|\alpha, \beta) = \dfrac{\beta^{\alpha}}{\Gamma(\alpha)}y^{-(\alpha + 1)}exp\left(-\beta \dfrac{1}{y}\right)$
$\text{serum creatinine} \sim  InvGamma(1,1)$

Ejection fraction is constrained to be from 0 to 100, as its expressed in percentage. For this reason we need to use beta distribution. We chose to parametrize beta distribution as (6,4), because the ejection fraction should be between 50 and 60. (6,4) parametrization provides us mean of 0.6.
$\text{ejection fraction} \sim Beta(y|\alpha, \beta) \dfrac{1}{B(\alpha, \beta)}\theta^{\alpha - 1}(1-\theta)^{\beta-1}$
$\text{ejection fraction} \sim Beta(6,4)$

Serum sodium should be around 135 and 145, but we gave it large variance with (half) cauchy(0,200). Severe symptoms of too high sodium levels start above 160, so this prior gives a lot room to reach fatal levels. 
$\text{serum sodium} \sim Cauchy(y|\mu,\sigma) = \dfrac{1}{\pi\sigma}\dfrac{1}{1+((y-\mu)/\sigma)^2}$
$\text{serum sodium} \sim Cauch(0,200)$

Priors for anemia, smoking, high blood pressure and sex were chosen as uniform beta(1,1). This is because they might get values 0 or 1 with equal probabilities.  
$\text{anemia, smoking, high blood pressure, sex} \sim \mathcal{N}(.5,.5)$  


**Intercept distribution**
Intercept distribution was student_t for every coefficient. The student_t distribution is parametrized by default as student_t(3, 0, 2.5).
$\text{Intercept} \sim StudentT(y|\upsilon, \mu, \sigma) = \dfrac{\Gamma((\upsilon+1)/2)}{\Gamma(\upsilon/2)}\left(1+\dfrac{1}{\upsilon}\left(\dfrac{y-\mu}{\sigma}\right)^2\right)^{-(\upsilon+1)/2}$
$\text{Intercept} \sim StudentT(3,0$

The types of priors we considered in this project are uninformative priors and regularizing priors. Surely this dataset is not the only one about heart failure but since no prior knowledge are provided in the original papers we opted for using uninformative priors. The domain is also highly specialized, as the target group is specific group of people over 40 years who have gone through ventricular systolic dysfunction. We can't simply infer any prior knowledge on this group. 

By default BRMS uses improper flat prior over the reals for population level parameters. Group level parameter is assumed to come from multivariate normal distribution with zero mean and unknown covariance matrix. 

## $\hat{R}$ convergence 

### Summary for the linear model with all variables 
```{r}
rhats <- rhat(fit.full)
color_scheme_set("brightblue") # see help("color_scheme_set")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for model with all variables") + theme(plot.title = element_text(hjust = 0.5))
```


### Summary for the linear model with selected variables 

```{r}
rhats <- rhat(fit.feature_selected)
color_scheme_set("brightblue") # see help("color_scheme_set")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for model with selected variables") + theme(plot.title = element_text(hjust = 0.5))
```

### Summary for the hierarchical model with all variables 
```{r}
rhats <- rhat(fit.hierarchical)
color_scheme_set("brightblue") # see help("color_scheme_set")
mcmc_rhat(rhats) + yaxis_text(hjust = 1) + ggtitle("Rhat values for hierarchical model with all variables") + theme(plot.title = element_text(hjust = 0.5))
```

From the model summaries, it is observable that $\hat{R}$ values for all models are around 1.0 which is below the threshold value of 1.05 as mentioned in Stan documentation. With this information, we can interpret that the chains have mixed well and the samples are reliable.


## HMC specific convergence diagnostics (divergences, tree depth) with interpretation of the results

### Feature selected model

```{r}
check_divergences(fit.feature_selected$fit)
```

```{r}
check_treedepth(fit.feature_selected$fit)
```

### Full model 

```{r}
check_divergences(fit.full$fit)
```

```{r}
check_treedepth(fit.full$fit)
```

### Hierarchical model

```{r}
check_divergences(fit.hierarchical$fit)
```

```{r}
check_treedepth(fit.hierarchical$fit)
```

### Weibull model

```{r}
check_divergences(fit.weibull$fit)
```

```{r}
check_treedepth(fit.weibull$fit)
```

A detailed comparison is carried at the end of the "Model comparison and interpretation of the results" section.

## Effective sample size diagnostic (n_eff or ESS)

### Feature selected model
```{r}
s<-summary(fit.feature_selected)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.feature_selected)
mcmc_neff_hist(ratio)
```

### Full model
```{r}
s<-summary(fit.full)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.full)
mcmc_neff_hist(ratio)
```

### Hierarchical model
```{r}
s<-summary(fit.hierarchical)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.hierarchical)
mcmc_neff_hist(ratio)
```

### Weibul model
```{r}
s<-summary(fit.weibull)
s[["fixed"]][,6:7]
ratio<-neff_ratio(fit.weibull)
mcmc_neff_hist(ratio)
```

A detailed comparison is carried at the end of the "Model comparison and interpretation of the results" section.

## Posterior predictive checking

```{r}
pp_check(fit.feature_selected, nsamples=50, newdata = test.data)
```

```{r}
pp_check(fit.full, nsamples=50, newdata = test.data)
```

```{r}
test.data$age <- discretize.variable(test.data$age)
train.data$age <- discretize.variable(train.data$age)
pp_check(fit.hierarchical, nsamples=50, newdata = test.data)
```

```{r}
new.data <- split.train.test(heart)
train.data <- new.data$train
test.data <- new.data$test
pp_check(fit.weibull, nsamples=50, newdata = test.data)
```

<<<<<<< HEAD
A detailed comparison is carried at the end of the "Model comparison and interpretation of the results" section.

=======
\newpage
>>>>>>> 640e8aa594c5e31fd15526b99c30b7de190d7cc5
## Model comparison and interpretation of the results

### Full model

```{r}
loo.full <-loo(fit.full)

hist(loo.full$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values", 
     ylab = "Frequency", freq = FALSE)
```

### Feature selected model

```{r}
loo.feature_selected <-loo(fit.feature_selected)

hist(loo.feature_selected$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values", 
     ylab = "Frequency", freq = FALSE)
```

### Hierarchical model
```{r}
loo.hierarchical <- loo(fit.hierarchical)

hist(loo.hierarchical$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values", 
     ylab = "Frequency",
     freq = FALSE)
```

### Weibull model

```{r}
loo.weibull <- loo(fit.weibull)

hist(loo.weibull$diagnostics$pareto_k, main = "Diagnostic histogram of Pareto k",  xlab = "k-values",
     ylab = "Frequency",
     freq = FALSE)
```

Model comparison: 

```{r}
loo_compare(list(loo.feature_selected, loo.full, loo.hierarchical))
```

Leave-one-out cross-validation is a method for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values [reference to aki paper].
From the PSIS_LOO we can see that the hierarchical model has the highest value compared to the other two models. This value is an indicator of model performance and the best model would be the one with the highest PSIS-LOO. Using the "loo" package we are able to estimate the difference in the models expected predictive accuracy and the function "loo_compare" creates a chart of the models with the one with highest ELPD (smallest LOOIC) first with zero values, meaning that is the best. We can see that the hierarchical model outstand the others and that there is a noticeable difference between the hierarchical model and the other models.
Pareto k-values estimates the tail shape which determines the convergence rate of PSIS. K-values less than 0.7 are consdered to be a measure of reliability for the models in question. Values greater than 0.7 could represent a problem for the models, are further evaluation need to be done. [comment our kvalues]

The effective sample size (ESS) measures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample. In other words the effective sample size is an estimate of the number of independent draws from the posterior distribution of the estimand of interest. The $n_{eff}$ metric used in Stan is based on the ability of the draws to estimate the true mean value of the parameter. Usually smaller than the total sample size $N$ so the larger the ratio to $N$, the better. [comment our neff]

## WIP: Predictive performance assessment (classification)

For actually seeing how the model performs, we split the data in to train and test sets. 
We have to use different train and test test for the hierarchical, but otherwise the pointwise testing is straigth forward

Function for predicting pointwise accuracy
```{r}
predict.pointwise.accuracy <- function(fitted.model, test.data) {
  preds <- round(predict(fitted.model, newdata = test.data)[,1])
  preds.correct <- preds == test.data$DEATH_EVENT

  pointwise.accuracy <- length(preds.correct[preds.correct == TRUE])/nrow(test.data)
  
  return(pointwise.accuracy)
}
```

### 

## Prior sensitivity analysis (alternative prior tested)

## Discussion of problems and further improvements

# Conclusion

## Self-reflection

We learned to work as a group. We used GitHub for version management, and it was nice to see that with communication we had zero conflicts, even though we only worked on main-branch. Seems like we also stayed on time perfectly. At first we were having group meetings really early, but it was worth it because we had longer time to think about the problem and possible solutions. 

\newpage
# Appendix {-}

## A. Stan code of full model {-}
```{r}
stancode(fit.full)
```

\newpage
## B. Stan feature selected model {-}
```{r}
stancode(fit.feature_selected)
```

\newpage
## C. Stan hierarhical model {-}
```{r}
stancode(fit.hierarchical)
```

\newpage
## D. Stan death time analysis model
```{r}
stancode(fit.weibull)
```

\newpage
# References
